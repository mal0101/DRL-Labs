{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca47acf3",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning Lab1: FrozenLake: A Slippery Challenge\n",
    "\n",
    "FrozenLake is a classic reinforcement learning environment provided by the OpenAI Gym library. It\n",
    "simulates the task of navigating a treacherous, icy terrain. The agent's goal is to reach the goal state\n",
    "without falling into any holes.\n",
    "\n",
    "**Key Characteristics:**\n",
    "\n",
    "- Grid World: The environment is represented as a 4x4 grid, with each cell representing a different state.\n",
    "- Actions: The agent can take four actions: up, down, left, or right.\n",
    "- State Transitions: Due to the slippery nature of the ice, the agent's actions may not always have the desired effect.\n",
    "- Rewards: The agent receives a reward of 1 upon reaching the goal state and a reward of 0 otherwise.\n",
    "- Terminal States: The goal state and hole states are terminal states.\n",
    "\n",
    "FrozenLake is a popular choice for introducing reinforcement learning concepts due to its simplicity\n",
    "and the challenge it presents. It's a great environment to experiment with various reinforcement\n",
    "learning algorithms, including Q-learning.\n",
    "\n",
    "For optimal performance and to avoid potential conflicts with system-wide Python\n",
    "installations, it is recommended to create a virtual environment using Anaconda. This will\n",
    "provide a dedicated Python environment for your project.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd965aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6341c577",
   "metadata": {},
   "source": [
    "Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cd33ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a480a1",
   "metadata": {},
   "source": [
    "Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de702bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.9 # exploration rate\n",
    "total_episodes = 100 # total episodes\n",
    "max_steps = 100 # max steps per episode\n",
    "alpha = 0.85 # learning rate\n",
    "gamma = 0.95 # discounting rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659c0d6b",
   "metadata": {},
   "source": [
    "Initializing Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a1ef29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros((env.observation_space.n, env.action_space.n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f6fd63",
   "metadata": {},
   "source": [
    "Implementing the `choose_action` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6412b59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state):\n",
    "    if np.random.uniform(0,1) < epsilon:\n",
    "        return env.action_space.sample() # Explore randomly\n",
    "    else:\n",
    "        return np.argmax(Q[state,:]) # Exploit best known action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40ee8a6",
   "metadata": {},
   "source": [
    "Implementing the `update` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98676bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(state, state2, reward, action):\n",
    "    predict = Q[state, action]\n",
    "    target = reward + gamma * np.max(Q[state2, :])\n",
    "    Q[state, action] = Q[state, action] + alpha * (target - predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59be6d5c",
   "metadata": {},
   "source": [
    "Implementing the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da6e938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(total_episodes):\n",
    "    state, _ = env.reset()\n",
    "    for t in range(max_steps):\n",
    "        action = choose_action(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        update(state, next_state, reward, action)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fee339",
   "metadata": {},
   "source": [
    "Evaluating Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1b01d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward over 100 episodes: 0.03\n"
     ]
    }
   ],
   "source": [
    "total_reward = 0\n",
    "for _ in range(100):\n",
    "    state, _ = env.reset() \n",
    "    while True:\n",
    "        action = np.argmax(Q[state, :])\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "print(\"Average reward over 100 episodes:\", total_reward / 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c90351a",
   "metadata": {},
   "source": [
    "Q-table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03f57e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.48028602e-01 3.61800634e-01 3.49068484e-01 3.62110606e-01]\n",
      " [1.93446707e-05 3.79669555e-01 3.81893988e-01 3.51849937e-01]\n",
      " [3.93749542e-01 5.47619053e-01 2.95210768e-01 3.95286947e-01]\n",
      " [4.38618316e-02 5.59971922e-03 4.52619839e-02 3.94446833e-01]\n",
      " [3.68161768e-01 1.22825905e-03 3.71477001e-01 5.70738134e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [6.19438318e-01 6.08457925e-01 2.46303202e-01 6.42167579e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [4.90998115e-03 2.51200753e-01 2.87975109e-01 4.39284490e-01]\n",
      " [8.66790651e-02 3.56592812e-01 2.94693882e-01 0.00000000e+00]\n",
      " [1.00699899e-02 7.53508266e-01 4.91329774e-01 2.37965310e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 8.53958830e-01 1.15804884e-02]\n",
      " [0.00000000e+00 9.89970576e-01 8.50000000e-01 7.08696755e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(Q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
